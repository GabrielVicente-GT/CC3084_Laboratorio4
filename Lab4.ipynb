{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 4\n",
    "### Laboratorio: Mejorando el Análisis de Sentimientos con LSTM y Características Adicionales\n",
    "* Objetivo: Incrementar la precisión en el análisis de sentimientos sobre las críticas de películas\n",
    "utilizando RNNs con unidades LSTM y la incorporación de características (features) adicionales.\n",
    "\n",
    "# Integrantes\n",
    "\n",
    "* Andrea Lam\n",
    "* Gabriel Vicente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importación y Pre-procesamiento (30 puntos)\n",
    "• Correcta importación del conjunto de datos con 50,000 palabras más frecuentes:\n",
    "10 puntos. <br/>\n",
    "• Secuenciación y relleno de las críticas: 10 puntos.<br/>\n",
    "• Extracción y adecuada justificación de características adicionales: 10 puntos.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/1b/66/2f47c39cfedb29188d82555d0184a619a0bf8234fd5e5301940efb0aa464/tensorflow-2.13.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.13.0-cp39-cp39-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.13.0 from https://files.pythonhosted.org/packages/2b/ad/d3a2e335004d178e0599cf8aff6c2a92cd21eb9789358fb8f3f951009930/tensorflow_intel-2.13.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_intel-2.13.0-cp39-cp39-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.1.21 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/a0/62/9790f98aa125a035cda91be7a41a46bdc76b26ffdd2ad2d3c5b7f7232946/h5py-3.9.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Using cached h5py-3.9.0-cp39-cp39-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/02/8c/dc970bc00867fe290e8c8a7befa1635af716a9ebdfe3fb9dce0ca4b522ce/libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata\n",
      "  Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\andre\\onedrive\\documentos\\github\\cc3084_laboratorio4\\.venv\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/ad/6d/6cc9491378d35f10b133b2677027eb08e97ca4b5c53edf6342fe8cf58d38/protobuf-4.24.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.24.3-cp39-cp39-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\onedrive\\documentos\\github\\cc3084_laboratorio4\\.venv\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\andre\\onedrive\\documentos\\github\\cc3084_laboratorio4\\.venv\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached wrapt-1.15.0-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/f3/b5/f4c2f0495007a955953d88119c428e4b14868caba2db585382e34074075f/grpcio-1.58.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.58.0-cp39-cp39-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for wheel<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/b8/8b/31273bf66016be6ad22bb7345c37ff350276cfd46e389a0c2ac5da9d9073/wheel-0.41.2-py3-none-any.whl.metadata\n",
      "  Using cached wheel-0.41.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9d/44/5a992cb9d7bf8aaae73bc5adaf721ad08731c9d00c1c17999a8691404b0c/google_auth-2.23.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.23.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Using cached Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for requests<3,>=2.21.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/da/61/6e9ff8258422d287eec718872fb71e05324356722ab658c8afda25f51539/tensorboard_data_server-0.7.1-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard_data_server-0.7.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/9b/59/a7c32e3d8d0e546a206e0552a2c04444544f15c1da4a01df8938d20c6ffc/werkzeug-2.3.7-py3-none-any.whl.metadata\n",
      "  Using cached werkzeug-2.3.7-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for urllib3<2.0 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\andre\\onedrive\\documentos\\github\\cc3084_laboratorio4\\.venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (6.8.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/cb/dd/dce14328e6abe0f475e606131298b4c8f628abd62a4e6f27fdfa496b9efe/charset_normalizer-3.2.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Using cached charset_normalizer-3.2.0-cp39-cp39-win_amd64.whl.metadata (31 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for MarkupSafe>=2.1.1 from https://files.pythonhosted.org/packages/a2/b2/624042cb58cc6b3529a6c3a7b7d230766e3ecb768cba118ba7befd18ed6f/MarkupSafe-2.1.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Using cached MarkupSafe-2.1.3-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\andre\\onedrive\\documentos\\github\\cc3084_laboratorio4\\.venv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.17.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached tensorflow-2.13.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Using cached tensorflow_intel-2.13.0-cp39-cp39-win_amd64.whl (276.5 MB)\n",
      "Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "   ---------------------------------------- 0.0/130.2 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 30.7/130.2 kB 1.3 MB/s eta 0:00:01\n",
      "   --------------------- ----------------- 71.7/130.2 kB 787.7 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 122.9/130.2 kB 901.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 130.2/130.2 kB 850.7 kB/s eta 0:00:00\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading grpcio-1.58.0-cp39-cp39-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/4.3 MB 9.0 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.4/4.3 MB 4.6 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.5/4.3 MB 3.8 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.7/4.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.8/4.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.3/4.3 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.8/4.3 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.2/4.3 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.6/4.3 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.9/4.3 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.9/4.3 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 3.3/4.3 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.9/4.3 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.3/4.3 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 6.4 MB/s eta 0:00:00\n",
      "Using cached h5py-3.9.0-cp39-cp39-win_amd64.whl (2.7 MB)\n",
      "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Downloading protobuf-4.24.3-cp39-cp39-win_amd64.whl (430 kB)\n",
      "   ---------------------------------------- 0.0/430.5 kB ? eta -:--:--\n",
      "   --------------------------------------  430.1/430.5 kB 28.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 430.5/430.5 kB 8.9 MB/s eta 0:00:00\n",
      "Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Downloading google_auth-2.23.0-py2.py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.4/181.4 kB 10.7 MB/s eta 0:00:00\n",
      "Using cached Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "Using cached wheel-0.41.2-py3-none-any.whl (64 kB)\n",
      "Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Using cached charset_normalizer-3.2.0-cp39-cp39-win_amd64.whl (96 kB)\n",
      "Using cached MarkupSafe-2.1.3-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, MarkupSafe, keras, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, opt-einsum, markdown, h5py, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "Successfully installed MarkupSafe-2.1.3 absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.1 certifi-2023.7.22 charset-normalizer-3.2.0 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.23.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.58.0 h5py-3.9.0 idna-3.4 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.24.3 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 typing-extensions-4.5.0 urllib3-1.26.16 werkzeug-2.3.7 wheel-0.41.2 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Tensor flow \"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando los datos...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "print('Cargando los datos...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen = 80)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Arquitectura del Modelo (30 puntos)\n",
    "• Adecuado diseño de la arquitectura LSTM: 10 puntos.<br/>\n",
    "• Incorporación efectiva de características adicionales en el modelo: 10 puntos.<br/>\n",
    "• Uso de técnicas adicionales (e.g., Dropout, capas densamente conectadas): 10\n",
    "puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelo original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_original = Sequential()\n",
    "modelo_original.add(Embedding(50000, 128))\n",
    "modelo_original.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelo_original.add(Dense(1, activation='sigmoid'))\n",
    "modelo_original.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelo con mejoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, MaxPooling1D\n",
    "\n",
    "modelo = Sequential()\n",
    "modelo.add(Embedding(50000, 128, input_length=80))  # Asegúrate de especificar input_length\n",
    "modelo.add(Dropout(0.2))  # Regularización Dropout en la capa de embedding\n",
    "\n",
    "# Capas Convolucionales\n",
    "modelo.add(Conv1D(64, 5, activation='relu'))\n",
    "modelo.add(MaxPooling1D(pool_size=4))\n",
    "modelo.add(Dropout(0.2))  # Regularización Dropout después de las capas convolucionales\n",
    "\n",
    "# Capas LSTM\n",
    "modelo.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "modelo.add(Dropout(0.2))\n",
    "modelo.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelo.add(Dropout(0.2))\n",
    "\n",
    "# Capa de salida\n",
    "modelo.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "modelo.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Entrenamiento y Evaluación (20 puntos)\n",
    "• Correcto entrenamiento del modelo sin errores: 10 puntos. <br/>\n",
    "• Evaluación y comparación adecuada con el modelo del ejercicio anterior: 10\n",
    "puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelo con cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "782/782 - 99s - loss: 0.4218 - accuracy: 0.7992 - val_loss: 0.3412 - val_accuracy: 0.8486 - 99s/epoch - 126ms/step\n",
      "Epoch 2/15\n",
      "782/782 - 97s - loss: 0.2089 - accuracy: 0.9201 - val_loss: 0.4141 - val_accuracy: 0.8346 - 97s/epoch - 124ms/step\n",
      "Epoch 3/15\n",
      "782/782 - 99s - loss: 0.0925 - accuracy: 0.9671 - val_loss: 0.4574 - val_accuracy: 0.8310 - 99s/epoch - 127ms/step\n",
      "Epoch 4/15\n",
      "782/782 - 100s - loss: 0.0429 - accuracy: 0.9857 - val_loss: 0.6319 - val_accuracy: 0.8147 - 100s/epoch - 128ms/step\n",
      "Epoch 5/15\n",
      "782/782 - 101s - loss: 0.0242 - accuracy: 0.9920 - val_loss: 0.7781 - val_accuracy: 0.8140 - 101s/epoch - 129ms/step\n",
      "Epoch 6/15\n",
      "782/782 - 102s - loss: 0.0174 - accuracy: 0.9939 - val_loss: 0.9336 - val_accuracy: 0.8129 - 102s/epoch - 131ms/step\n",
      "Epoch 7/15\n",
      "782/782 - 101s - loss: 0.0161 - accuracy: 0.9944 - val_loss: 1.0608 - val_accuracy: 0.8076 - 101s/epoch - 130ms/step\n",
      "Epoch 8/15\n",
      "782/782 - 102s - loss: 0.0122 - accuracy: 0.9953 - val_loss: 1.0909 - val_accuracy: 0.8119 - 102s/epoch - 130ms/step\n",
      "Epoch 9/15\n",
      "782/782 - 103s - loss: 0.0150 - accuracy: 0.9952 - val_loss: 0.8440 - val_accuracy: 0.8089 - 103s/epoch - 131ms/step\n",
      "Epoch 10/15\n",
      "782/782 - 102s - loss: 0.0100 - accuracy: 0.9968 - val_loss: 0.8796 - val_accuracy: 0.8149 - 102s/epoch - 130ms/step\n",
      "Epoch 11/15\n",
      "782/782 - 102s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.8964 - val_accuracy: 0.8122 - 102s/epoch - 130ms/step\n",
      "Epoch 12/15\n",
      "782/782 - 102s - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.9523 - val_accuracy: 0.8100 - 102s/epoch - 131ms/step\n",
      "Epoch 13/15\n",
      "782/782 - 103s - loss: 0.0053 - accuracy: 0.9985 - val_loss: 1.1967 - val_accuracy: 0.8100 - 103s/epoch - 131ms/step\n",
      "Epoch 14/15\n",
      "782/782 - 105s - loss: 0.0056 - accuracy: 0.9983 - val_loss: 1.0177 - val_accuracy: 0.8109 - 105s/epoch - 134ms/step\n",
      "Epoch 15/15\n",
      "782/782 - 101s - loss: 0.0060 - accuracy: 0.9977 - val_loss: 1.2729 - val_accuracy: 0.8090 - 101s/epoch - 129ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1efda7539a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(X_train, y_train,batch_size = 32,epochs = 15,verbose = 2,validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelo original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "782/782 - 140s - loss: 0.3359 - accuracy: 0.8576 - val_loss: 0.3989 - val_accuracy: 0.8208 - 140s/epoch - 180ms/step\n",
      "Epoch 2/15\n",
      "782/782 - 139s - loss: 0.1902 - accuracy: 0.9276 - val_loss: 0.4003 - val_accuracy: 0.8291 - 139s/epoch - 178ms/step\n",
      "Epoch 3/15\n",
      "782/782 - 140s - loss: 0.1087 - accuracy: 0.9608 - val_loss: 0.5350 - val_accuracy: 0.8223 - 140s/epoch - 180ms/step\n",
      "Epoch 4/15\n",
      "782/782 - 140s - loss: 0.0604 - accuracy: 0.9796 - val_loss: 0.6311 - val_accuracy: 0.8126 - 140s/epoch - 179ms/step\n",
      "Epoch 5/15\n",
      "782/782 - 140s - loss: 0.0510 - accuracy: 0.9826 - val_loss: 0.6698 - val_accuracy: 0.8051 - 140s/epoch - 179ms/step\n",
      "Epoch 6/15\n",
      "782/782 - 143s - loss: 0.0328 - accuracy: 0.9894 - val_loss: 0.9203 - val_accuracy: 0.8107 - 143s/epoch - 183ms/step\n",
      "Epoch 7/15\n",
      "782/782 - 143s - loss: 0.0234 - accuracy: 0.9926 - val_loss: 0.8296 - val_accuracy: 0.8128 - 143s/epoch - 183ms/step\n",
      "Epoch 8/15\n",
      "782/782 - 140s - loss: 0.0156 - accuracy: 0.9950 - val_loss: 0.9951 - val_accuracy: 0.8115 - 140s/epoch - 178ms/step\n",
      "Epoch 9/15\n",
      "782/782 - 138s - loss: 0.0141 - accuracy: 0.9958 - val_loss: 1.1428 - val_accuracy: 0.8054 - 138s/epoch - 176ms/step\n",
      "Epoch 10/15\n",
      "782/782 - 135s - loss: 0.0229 - accuracy: 0.9928 - val_loss: 0.9705 - val_accuracy: 0.8050 - 135s/epoch - 172ms/step\n",
      "Epoch 11/15\n",
      "782/782 - 133s - loss: 0.0132 - accuracy: 0.9959 - val_loss: 0.9797 - val_accuracy: 0.8113 - 133s/epoch - 171ms/step\n",
      "Epoch 12/15\n",
      "782/782 - 134s - loss: 0.0053 - accuracy: 0.9984 - val_loss: 1.1490 - val_accuracy: 0.8096 - 134s/epoch - 171ms/step\n",
      "Epoch 13/15\n",
      "782/782 - 134s - loss: 0.0037 - accuracy: 0.9990 - val_loss: 1.2529 - val_accuracy: 0.8090 - 134s/epoch - 171ms/step\n",
      "Epoch 14/15\n",
      "782/782 - 134s - loss: 0.0062 - accuracy: 0.9979 - val_loss: 1.0651 - val_accuracy: 0.8054 - 134s/epoch - 171ms/step\n",
      "Epoch 15/15\n",
      "782/782 - 134s - loss: 0.0087 - accuracy: 0.9973 - val_loss: 1.2061 - val_accuracy: 0.8023 - 134s/epoch - 171ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1efe4483370>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_original.fit(X_train, y_train,batch_size = 32,epochs = 15,verbose = 2,validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelo con cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 7s - loss: 1.2729 - accuracy: 0.8090 - 7s/epoch - 9ms/step\n",
      "╒═══════════════╤═════════════╕\n",
      "│ Métrica       │ Resultado   │\n",
      "╞═══════════════╪═════════════╡\n",
      "│ Loss          │ 1.2729      │\n",
      "├───────────────┼─────────────┤\n",
      "│ Test Accuracy │ 80.90%      │\n",
      "╘═══════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "loss, accuracy = modelo.evaluate(X_test, y_test, batch_size=32, verbose=2)\n",
    "\n",
    "# Definir los títulos y resultados\n",
    "headers = [\"Métrica\", \"Resultado\"]\n",
    "data = [[\"Loss\", f\"{loss:.4f}\"],\n",
    "        [\"Test Accuracy\", f\"{accuracy * 100:.2f}%\"]]\n",
    "\n",
    "# Imprimir la tabla utilizando tabulate\n",
    "table = tabulate(data, headers, tablefmt=\"fancy_grid\")\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelo original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 10s - loss: 1.2061 - accuracy: 0.8023 - 10s/epoch - 12ms/step\n",
      "╒═══════════════╤═════════════╕\n",
      "│ Métrica       │ Resultado   │\n",
      "╞═══════════════╪═════════════╡\n",
      "│ Loss          │ 1.2061      │\n",
      "├───────────────┼─────────────┤\n",
      "│ Test Accuracy │ 80.23%      │\n",
      "╘═══════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "loss, accuracy = modelo_original.evaluate(X_test, y_test, batch_size=32, verbose=2)\n",
    "\n",
    "# Definir los títulos y resultados\n",
    "headers = [\"Métrica\", \"Resultado\"]\n",
    "data = [[\"Loss\", f\"{loss:.4f}\"],\n",
    "        [\"Test Accuracy\", f\"{accuracy * 100:.2f}%\"]]\n",
    "\n",
    "# Imprimir la tabla utilizando tabulate\n",
    "table = tabulate(data, headers, tablefmt=\"fancy_grid\")\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.save(\"Lab4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Informe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clara descripción de características adicionales y su relevancia: 5 puntos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agregaron capas de Dropout después de la capa de embedding, después de las capas convolucionales y después de las capas LSTM para aplicar regularización y reducir el sobreajuste.\n",
    "\n",
    "Se añadieron capas CNN para capturar patrones locales en el texto antes de las capas LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación coherente de la arquitectura del modelo: 5 puntos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo está configurado para minimizar la función de pérdida de entropía cruzada binaria ('binary_crossentropy') utilizando el optimizador 'adam', y evalúa su rendimiento en términos de precisión ('accuracy'). Esta arquitectura combina capas de embedding, convolucionales y LSTM para procesar secuencias de texto y aprender representaciones significativas de las palabras, lo que permite la clasificación binaria de los sentimientos en textos. La regularización y las capas Dropout ayudan a prevenir el sobreajuste durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Presentación de resultados y comparativa efectiva: 10 puntos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en el modelo original se tuvo un menor loss del 1.2061, mientras que el loss del modelo mejora fue de 1.2729; esto se puede deber a diferentes razones como la arquitectura del modelo, los hiperparametros, la inilicacion de pesos o la regularización.\n",
    "Pero por otro lado el accuracy fue mejor para el modelo mejorado ya que obtuvo un 80.90% mientras el modelo original tiene un 80.23%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
